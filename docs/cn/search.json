[
  {
    "objectID": "Ollama.html",
    "href": "Ollama.html",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "",
    "text": "Ollama 是一个强大的工具，允许你在本地计算机上运行大型语言模型（LLMs）。这为隐私保护和离线使用提供了可能。ollamar 是一个为 Ollama 提供简单直观接口的 R 包，方便你在 R 项目中轻松使用 LLM 的强大能力。\n本指南将通过示例讲解如何使用 ollamar 包进行文本生成、图像分析等任务。"
  },
  {
    "objectID": "Ollama.html#下载并运行-ollama",
    "href": "Ollama.html#下载并运行-ollama",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "2.1 下载并运行 Ollama",
    "text": "2.1 下载并运行 Ollama\n在使用该包之前，请确保你已经下载了最新版本的 Ollama 并在本地计算机上运行。你可以从 Ollama 官方网站 下载。"
  },
  {
    "objectID": "Ollama.html#测试连接",
    "href": "Ollama.html#测试连接",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "2.2 测试连接",
    "text": "2.2 测试连接\nOllama 运行后，你可以在 R 中测试连接以确保设置正确。\n\n\nCode\n# 测试与 Ollama 的连接\ntest_connection()"
  },
  {
    "objectID": "Ollama.html#列出已下载模型",
    "href": "Ollama.html#列出已下载模型",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "3.1 列出已下载模型",
    "text": "3.1 列出已下载模型\n你可以查看所有本地已下载的模型。\n\n\nCode\n# 列出所有本地可用模型\nollamar::list_models()"
  },
  {
    "objectID": "Ollama.html#下载新模型",
    "href": "Ollama.html#下载新模型",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "3.2 下载新模型",
    "text": "3.2 下载新模型\n你可以从 Ollama 库中轻松下载新模型。下面示例下载的是适用于文本与图像任务的 gemma:2b 模型。\n\n\nCode\n# 下载 gemma:2b 模型\nollamar::pull(\"gemma:2b\")\n\n\n你也可以查看模型的详细信息。 ::: {.cell}\n\nCode\n# 显示模型信息\n#ollamar::show(\"gemma:2b\")\n\n:::"
  },
  {
    "objectID": "Ollama.html#删除模型",
    "href": "Ollama.html#删除模型",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "3.3 删除模型",
    "text": "3.3 删除模型\n如果不再需要某个模型，你可以将其删除以释放空间。\n\n\nCode\n# 通过名称和标签删除模型\ndelete(\"gemma:latest\")"
  },
  {
    "objectID": "Ollama.html#运行文本模型",
    "href": "Ollama.html#运行文本模型",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "4.1 运行文本模型",
    "text": "4.1 运行文本模型\n你可以使用 generate 函数与文本模型进行交互。\n\n4.1.1 使用 gemma:2b 模型\n\n\nCode\n# 使用 gemma:2b 模型生成一则短故事\n# 参数 'output = \"text\"' 表示输出为字符字符串\ntxt &lt;- generate(\"gemma:2b\", \"Tell me a 50-word story\", output = \"text\")\ntxt"
  },
  {
    "objectID": "Ollama.html#运行图像模型",
    "href": "Ollama.html#运行图像模型",
    "title": "Ollama：R 的本地大语言模型包",
    "section": "4.2 运行图像模型",
    "text": "4.2 运行图像模型\n现代多模态模型还可以分析图像。\n\n\n\nCode\n# 定义图像路径\nimage_path &lt;- \"coffee.jpeg\"\n# 让模型描述图像内容\nresult &lt;- generate(\"gemma:2b\", \"What is in the image?\", images = image_path)\n# 处理响应以提取文本\nresp_process(result, \"text\")\n\n\n你也可以就图像提出更具体的问题。\n\n\nCode\n# 询问图像中有多少杯咖啡\nresult &lt;- generate(\"gemma:2b\", \"How many coffees are in this image?\", images = image_path)\nresp_process(result, \"text\")\n\n\n\n\nCode\n# 询问图像的主色调\nresult &lt;- generate(\"gemma:2b\", \"What is the main color of this image?\", images = image_path)\nresp_process(result, \"text\")\n\n\n\n\nCode\n# 执行内容审核\nresult &lt;- generate(\"gemma:2b\", \"Is there any adult content in this image?\", images = image_path)\nresp_process(result, \"text\")"
  },
  {
    "objectID": "chatlas.html",
    "href": "chatlas.html",
    "title": "Chatlas：一个统一的 Python 大语言模型接口",
    "section": "",
    "text": "chatlas 是一个 Python 包，为多个大语言模型（LLM）提供了一个简单统一的交互接口。它允许你在不同模型和提供商之间轻松切换，仅需最少的代码更改，从而优化工作流程。\n本文档演示如何使用 chatlas 连接 Google 的 Gemini 模型，进行文本生成以及图像分析。"
  },
  {
    "objectID": "chatlas.html#token-计数",
    "href": "chatlas.html#token-计数",
    "title": "Chatlas：一个统一的 Python 大语言模型接口",
    "section": "3.1 Token 计数",
    "text": "3.1 Token 计数\n你可以在将文本发送给模型之前计算其 token 数量。这有助于管理成本并保持在模型的限制范围内。\n\n\nCode\n# 统计提示词中的 token 数\nchat_google_model.token_count(\"What preceding languages most influenced Python?\")"
  },
  {
    "objectID": "chatlas.html#文本生成",
    "href": "chatlas.html#文本生成",
    "title": "Chatlas：一个统一的 Python 大语言模型接口",
    "section": "3.2 文本生成",
    "text": "3.2 文本生成\n现在，让我们从模型生成一个响应。\n\n\nCode\n# 向模型发送提示词并获取响应\nresult = chat_google_model.chat(\"What preceding languages most influenced Python?\")\nresult"
  },
  {
    "objectID": "chatlas.html#网页应用",
    "href": "chatlas.html#网页应用",
    "title": "Chatlas：一个统一的 Python 大语言模型接口",
    "section": "4.1 网页应用",
    "text": "4.1 网页应用\n你可以启动一个基于网页的聊天应用。\n\n\nCode\n# 启动交互式网页应用\nchat_google_model.app()"
  },
  {
    "objectID": "chatlas.html#控制台模式",
    "href": "chatlas.html#控制台模式",
    "title": "Chatlas：一个统一的 Python 大语言模型接口",
    "section": "4.2 控制台模式",
    "text": "4.2 控制台模式\n或者，你可以直接在控制台中与模型对话。\n\n\nCode\n# 启动交互式控制台会话\nchat_google_model.console()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI 手册",
    "section": "",
    "text": "欢迎来到《AI 手册》，这是一个为希望利用人工智能力量的开发者和数据科学家提供的综合资源。这个基于 Quarto 的网站提供了一系列精选的教程、指南和实践示例，涵盖了如何在 R 和 Python 中使用各种 AI 工具和模型。\n我们的使命是让 AI 更易于使用，并更容易地集成到您的项目中。无论您是刚开始 AI 之旅的新手，还是经验丰富的从业者，您都可以在这里找到有价值的信息来帮助您提升技能。\n本手册涵盖了广泛的主题，包括：\n\nAI 基础概念： 清晰解释核心概念，如 AI 代理（AI Agents）、模型-控制器-解析器（MCP）设计模式以及检索增强生成（RAG）。\n用于 AI 的 R 包： 关于流行 R 包（如 ellmer、chattr 和 ollamar）的深入教程，可将大型语言模型（LLMs）无缝集成到您的 R 工作流程中。\n用于 AI 的 Python 库： 使用 Python 库的实用指南，例如使用 chatlas 与不同的 LLM 提供商获得统一体验，以及使用 mlx-lm 在苹果芯片（Apple silicon）上进行高性能模型执行。\n命令行工具： 如何使用像 gemini-cli 这样的命令行界面，直接从您的终端与谷歌 Gemini 等强大模型进行交互的说明。\n\n我们邀请您探索本手册的各个部分，以发现最新的人工智能技术，并学习如何将其应用于您自己的项目。凭借对动手学习和实际应用的关注，《AI 手册》是您探索激动人心的人工智能世界的首选指南。"
  },
  {
    "objectID": "gemini.html#升级-gemini-cli",
    "href": "gemini.html#升级-gemini-cli",
    "title": "开始使用 Gemini CLI",
    "section": "3.1 升级 Gemini CLI",
    "text": "3.1 升级 Gemini CLI\n为了确保拥有最新的功能和错误修复，可以不时地升级此包。\n\n\nCode\n# 将 Gemini CLI 升级到最新版本\nnpm upgrade -g @google/gemini-cli"
  },
  {
    "objectID": "gemini.html#使用-google-账户登录",
    "href": "gemini.html#使用-google-账户登录",
    "title": "开始使用 Gemini CLI",
    "section": "4.1 使用 Google 账户登录",
    "text": "4.1 使用 Google 账户登录\n你可以使用 Google Cloud 账户或 API 密钥登录。\n\n4.1.1 选项一：使用 Google Cloud 账户登录\n\n\nCode\n# 设置你的 Google Cloud 项目 ID\nexport GOOGLE_CLOUD_PROJECT=\"your-google-cloud-project-id\"\n\n\n或者将 GOOGLE_CLOUD_PROJECT 保存为环境变量，这样每次就不需要重新输入。\n\n4.1.1.1 检查你使用的是 zsh 还是 bash\n\n\nCode\necho $SHELL\n\n\n\n\n4.1.1.2 如果是 zsh：\n\n\nCode\necho 'export GOOGLE_CLOUD_PROJECT=\"your-google-cloud-project-id\"' &gt;&gt; ~/.zshrc\n\nsource ~/.zshrc\n\n\n\n\n4.1.1.3 如果是 bash：\n\n\nCode\necho 'export GOOGLE_CLOUD_PROJECT=\"your-google-cloud-project-id\"' &gt;&gt; ~/.bashrc\n\nsource ~/.bashrc\n\n\n\n\n4.1.1.4 检查是否成功添加：\n\n\nCode\necho $GOOGLE_CLOUD_PROJECT\n\n\n\n\n\n4.1.2 选项二：使用 API 密钥登录\n另外，你也可以使用 API 密钥进行身份验证。\n\n\nCode\n# 将你的 Gemini API 密钥设置为环境变量\nexport GEMINI_API_KEY=\"your-gemini-api-key\""
  },
  {
    "objectID": "gemini.html#设置位置",
    "href": "gemini.html#设置位置",
    "title": "开始使用 Gemini CLI",
    "section": "4.2 设置位置",
    "text": "4.2 设置位置\n你还需要指定 Google Cloud 中资源将被管理的地理位置。\n\n\nCode\n# 设置 Google Cloud 位置\nexport GOOGLE_CLOUD_LOCATION='us-central1'"
  },
  {
    "objectID": "chattr.html#加载包",
    "href": "chattr.html#加载包",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "3.1 加载包",
    "text": "3.1 加载包\n安装后，将 chattr 库加载到您的 R 会话中。\n\n\nCode\n# 加载 chattr 库\nlibrary(chattr)"
  },
  {
    "objectID": "chattr.html#设置聊天提供商",
    "href": "chattr.html#设置聊天提供商",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "3.2 设置聊天提供商",
    "text": "3.2 设置聊天提供商\nchattr 支持多种 LLM 提供商。在这里，我们通过 ellmer 包使用 GitHub 的 Copilot 模型来设置一个聊天提供商。\n\n\nCode\n# 使用 GitHub Copilot (gpt-4o 模型) 设置聊天提供商\n# 注意：GitHub Copilot 不需要单独的 OpenAI API 密钥。\nmy_chat &lt;- ellmer::chat_github(model = \"gpt-4o\")\nchattr_use(my_chat)"
  },
  {
    "objectID": "chattr.html#使用-chattr-应用",
    "href": "chattr.html#使用-chattr-应用",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "4.1 使用 chattr 应用",
    "text": "4.1 使用 chattr 应用\nchattr 包含一个基于 Shiny 的小工具，用于提供交互式聊天体验。\n\n\nCode\n# 启动交互式 chattr 应用\n# chattr_app()"
  },
  {
    "objectID": "chattr.html#在代码中直接互动",
    "href": "chattr.html#在代码中直接互动",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "4.2 在代码中直接互动",
    "text": "4.2 在代码中直接互动\n您也可以直接从 R 代码向 LLM 发送提示。\n\n\nCode\n# 向已配置的聊天提供商发送提示\nchattr(\"你叫什么名字？\")"
  },
  {
    "objectID": "chattr.html#查看历史记录",
    "href": "chattr.html#查看历史记录",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "5.1 查看历史记录",
    "text": "5.1 查看历史记录\n\n\nCode\n# 检索并打印聊天历史记录\nchattr_history &lt;- ch_history()\nprint(chattr_history)"
  },
  {
    "objectID": "chattr.html#保存历史记录",
    "href": "chattr.html#保存历史记录",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "5.2 保存历史记录",
    "text": "5.2 保存历史记录\n您可以将聊天历史记录保存到文件中以备后用。\n\n\nCode\n# 将当前聊天历史记录保存到 RDS 文件\nsaveRDS(ch_history(), \"chat_history.rds\")"
  },
  {
    "objectID": "chattr.html#清除历史记录",
    "href": "chattr.html#清除历史记录",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "5.3 清除历史记录",
    "text": "5.3 清除历史记录\n要开始一个新会话，您可以清除当前的聊天历史记录。\n\n\nCode\n# 通过传递一个空列表来清除聊天历史记录\nprint(ch_history(list()))"
  },
  {
    "objectID": "chattr.html#重新加载历史记录",
    "href": "chattr.html#重新加载历史记录",
    "title": "chattr：一个用于LLMs的RStudio/Positron接口",
    "section": "5.4 重新加载历史记录",
    "text": "5.4 重新加载历史记录\n您可以重新加载之前保存的聊天历史记录。\n\n\nCode\n# 从 RDS 文件加载已保存的聊天历史记录\nchattr_history &lt;- ch_history(readRDS(\"chat_history.rds\"))\nprint(chattr_history)"
  },
  {
    "objectID": "mlx_lm python.html#加载模型和分词器",
    "href": "mlx_lm python.html#加载模型和分词器",
    "title": "Python mlx-lm 入门指南",
    "section": "3.1 加载模型和分词器",
    "text": "3.1 加载模型和分词器\n我们将使用 mlx_lm 中的 load 函数来下载并加载模型及其对应的分词器。在本例中，我们使用的是来自 mlx-community Hugging Face 组织的 Mistral-7B-Instruct-v0.3 模型的 4 位量化版本。\n\n\nCode\nfrom mlx_lm import load, generate\n\n# 加载预训练模型和分词器\nmodel, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.3-4bit\")"
  },
  {
    "objectID": "mlx_lm python.html#生成文本",
    "href": "mlx_lm python.html#生成文本",
    "title": "Python mlx-lm 入门指南",
    "section": "3.2 生成文本",
    "text": "3.2 生成文本\n模型和分词器加载后，我们可以使用 generate 函数根据提示生成文本。\n\n\nCode\n# 定义提示\nprompt = \"写一个关于爱因斯坦的故事\"\n\n# 使用聊天模板格式化提示\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nformatted_prompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\n# 生成文本\ntext = generate(model, tokenizer, prompt=formatted_prompt, verbose=True)\ntext"
  },
  {
    "objectID": "ellmer.html#加载必要的包",
    "href": "ellmer.html#加载必要的包",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "3.1 加载必要的包",
    "text": "3.1 加载必要的包\n将 ellmer 和 keyring 包加载到您的 R 会话中。keyring 用于安全地管理 API 密钥。\n\n\nCode\n# 加载所需的库\nlibrary(ellmer)\nlibrary(keyring)"
  },
  {
    "objectID": "ellmer.html#初始化聊天模型",
    "href": "ellmer.html#初始化聊天模型",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "4.1 初始化聊天模型",
    "text": "4.1 初始化聊天模型\n首先，创建一个 Gemini 聊天模型的实例，提供您的 API 密钥并指定要使用的模型。\n\n\nCode\n# 设置 Google Gemini 聊天模型\nchat_gemini_model &lt;- chat_google_gemini(\n  api_key = key_get(\"google_ai_api_key\"),\n  model = \"gemini-1.5-flash\"\n)\n\nchat_gemini_model"
  },
  {
    "objectID": "ellmer.html#生成文本",
    "href": "ellmer.html#生成文本",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "4.2 生成文本",
    "text": "4.2 生成文本\n模型初始化后，您就可以开始聊天并生成文本。\n\n\nCode\n# 从模型生成回应\nresult &lt;- chat_gemini_model$chat(\"给我讲三个关于统计学家的笑话\")\nresult"
  },
  {
    "objectID": "ellmer.html#实时浏览器模式",
    "href": "ellmer.html#实时浏览器模式",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "5.1 实时浏览器模式",
    "text": "5.1 实时浏览器模式\n您可以启动一个基于 Web 的界面与模型聊天。\n\n\nCode\n# 在网页浏览器中打开交互式聊天会话\nlive_browser(chat_gemini_model)"
  },
  {
    "objectID": "ellmer.html#控制台模式",
    "href": "ellmer.html#控制台模式",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "5.2 控制台模式",
    "text": "5.2 控制台模式\n或者，您可以直接在 R 控制台中与模型聊天。\n\n\nCode\n# 在控制台中启动交互式聊天会话\nlive_console(chat_gemini_model)"
  },
  {
    "objectID": "ellmer.html#使用系统提示",
    "href": "ellmer.html#使用系统提示",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "6.1 使用系统提示",
    "text": "6.1 使用系统提示\n您可以提供系统提示来引导模型的行为和语气。\n\n\nCode\n# 定义一个系统提示\nsystem_prompt &lt;- \"你是一位IT专家\"\nsystem_prompt\n\n\n\n\nCode\n# 使用系统提示初始化模型\nchat_gemini_model_expert &lt;- chat_google_gemini(\n  system_prompt = system_prompt,\n  api_key = key_get(\"google_ai_api_key\"),\n  model = \"gemini-1.5-flash\"\n)\n\nchat_gemini_model_expert"
  },
  {
    "objectID": "ellmer.html#视觉能力",
    "href": "ellmer.html#视觉能力",
    "title": "ellmer：一个灵活的 R 语言 LLM 框架",
    "section": "6.2 视觉能力",
    "text": "6.2 视觉能力\nellmer 还支持可以分析图像的多模态模型。\n\n首先，将图像文件上传到 Google API。\n\n\nCode\n# 上传一个图像文件\nfile &lt;- google_upload(\n  path = \"coffee.jpeg\",\n  api_key = key_get(\"google_ai_api_key\")\n)\n\n\n然后，您可以要求模型描述或分析该图像。\n\n\nCode\n# 要求模型总结图像内容\nchat_gemini_model$chat(file, \"请用三段话总结一下这张图片的内容\")"
  },
  {
    "objectID": "LLM.html",
    "href": "LLM.html",
    "title": "人工智能基础概念",
    "section": "",
    "text": "AI 智能体是一个自主实体，它通过传感器感知其环境，并通过执行器对环境做出反应，以实现特定目标。在大型语言模型（LLM）的背景下，智能体可以被设计为执行任务、做出决策，并以目标导向的方式与用户或其他系统进行交互。\nAI 智能体的关键特征包括：\n\n自主性： 能够在无需人类干预的情况下运行。\n反应性： 能够感知并响应环境中的变化。\n主动性： 能主动采取行动以实现目标。\n目标导向性： 能够为达成特定目标采取行动。"
  },
  {
    "objectID": "LLM.html#rag-的工作原理",
    "href": "LLM.html#rag-的工作原理",
    "title": "人工智能基础概念",
    "section": "3.1 RAG 的工作原理",
    "text": "3.1 RAG 的工作原理\nRAG 的处理过程通常包含两个主要步骤：\n\n检索： 当用户提供提示词时，系统首先从知识库（如文档集合或数据库）中检索相关信息。这通常使用基于向量搜索的检索模型完成。\n生成： 检索到的信息会与原始提示词结合，并一同输入到 LLM 中。模型使用增强后的上下文生成更准确、最新且更符合语境的响应。\n\n通过将 LLM 的回答建立在可验证的信息基础上，RAG 有助于减少幻觉，提高生成内容的整体质量与可靠性。"
  },
  {
    "objectID": "LLM.html#rag-的优势",
    "href": "LLM.html#rag-的优势",
    "title": "人工智能基础概念",
    "section": "3.2 RAG 的优势",
    "text": "3.2 RAG 的优势\n\n提高准确性： 响应基于可信知识源中的事实信息。\n增强相关性： 生成的内容更具体，也更符合用户问题的上下文。\n减少幻觉： 模型更少生成虚假或误导性的信息。\n提升透明度： 通常可以追溯生成内容的来源文档，从而提供一定程度的可解释性。"
  },
  {
    "objectID": "LLM.html#参考资料",
    "href": "LLM.html#参考资料",
    "title": "人工智能基础概念",
    "section": "3.3 参考资料",
    "text": "3.3 参考资料\n\n什么是 RAG（检索增强生成）？\n检索增强生成（RAG）解析\n什么是检索增强生成？"
  }
]